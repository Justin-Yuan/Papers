# RL: Offline RL

---

## Offline Reinforcement Learning Workshop

link: [https://offline-rl-neurips.github.io/](https://offline-rl-neurips.github.io/) 

repo: [https://github.com/offline-rl-neurips/offline-rl-neurips.github.io](https://github.com/offline-rl-neurips/offline-rl-neurips.github.io) 

---

## Offline(Batch) reinforcement learning

link: [https://danieltakeshi.github.io/2020/06/28/offline-rl/](https://danieltakeshi.github.io/2020/06/28/offline-rl/)

summary

- how does one maximally exploit a static dataset ?
- offline RL disentanges exploration from exploitation, can provide standardized comparisons of the exploitation capability of RL algos
- related to imitation learning
- key overview [Batch Reinforcement Learning](http://tgabel.de/cms/fileadmin/user_upload/documents/Lange_Gabel_EtAl_RL-Book-12.pdf), [Levin's survey](https://arxiv.org/pdf/2005.01643.pdf)
- *extrapolation error*
    - state-action pairs $(s,a)$ outside the data batch an have arbitrarily inaccurate values
    - proposed **Batch Constrained deep Q-learning (BCQ)**
- need to evaluate Batch RL algos under *unified* settings
    - proposed discrete BCQ
    - train a behavior cloning network  $G_w(a|s)$ to predict actions of the behavior policy based on its states
    - minimize loss for offline policy

    $$L(\theta) = l_k\bigg(r + \gamma \Big( max_{a'\ s.t.\ \frac{G_w(a'|s')}{max_{\hat{a}}G_w(\hat{a}|s')}\ >\  \tau} Q_{\theta'}(s',a') \Big) - Q_{\theta}(s,a)\bigg)$$

    - for $\tau$ , if low → Q learning, if high → behavior cloning
- distributional RL aids exploitation ?
- Is it possible to run offline RL, and reliably exceed the noise-free behavior policy ?
- *bootstrapping error*
    - key source of instability
    - due to bootstrapping from actions that lie outside of the training data distribution, and it accumulates via the Bellman backup operator

        ![RL%20Offline%20RL%203fb412ab82474b19b1506a659d543f0e/s_03D8A88577B961181603AE5EDBD4A511CD8E828E7651B8AA640A61950DAB9783_1575540995462_problem.png](RL%20Offline%20RL%203fb412ab82474b19b1506a659d543f0e/s_03D8A88577B961181603AE5EDBD4A511CD8E828E7651B8AA640A61950DAB9783_1575540995462_problem.png)

    - carefully constraining the actions considered during Q-learning can mitigate error propagation
    - proposed **Bootstrapping Error Accumulation Reduction (BEAR)**
        - ensure learnted policy matches the *support* of the action distribution from the static data
        - in contrast to BCQ which focuses on *distribution matching*

        ![RL%20Offline%20RL%203fb412ab82474b19b1506a659d543f0e/Untitled.png](RL%20Offline%20RL%203fb412ab82474b19b1506a659d543f0e/Untitled.png)

        - enforce by using *Maximum Mean Discrepancy (MMD)* between actions from unknown behavior policy $\beta$ and actor $\pi$, estimated from samples

        BCQ is generally better when off-policy data is collected by an expert, but BEAR is better when data is collected by a weaker (or even random) policy. They claim this is because BCQ too aggressively constrains the distribution of actions, and this matches the interpretation of BCQ as matching the distribution of the policy of the data batch, whereas BEAR focuses on only matching the action support.

- improving the base off-policy deep RL algo can work well in Batch RL setting
    - propose **Random Ensemble Mixture (REM)**
        - use an ensemble of Q-networks and enforces Bellman consistency among random convex combinations

        ![RL%20Offline%20RL%203fb412ab82474b19b1506a659d543f0e/Untitled%201.png](RL%20Offline%20RL%203fb412ab82474b19b1506a659d543f0e/Untitled%201.png)

        - in Offline RL, the quality of the data matters significantly, and it is better to use data from many different policies rather than one fixed policy
- **IRIS: Implicit Reinforcement without Interaction at Scale**
    - offline learning from large-scale robotics datasets, where the demonstrations may be either suboptimal or highly multi-modal
    - goal proposals (high-level) → value-guided selection for goal (high-level) → action selection (low-level, conditioned on goal)
    - use Batch RL to only train a value function but not a policy
    - a mix of both the “action constraining” algorithms and the “learning from large scale datasets” papers
    - show that offline RL could be used as part of the process for robot manipulation
- DeepMind work on combining massive, offline dataset + strong off-policy RL algo

    ![RL%20Offline%20RL%203fb412ab82474b19b1506a659d543f0e/Never-Ending-Storage.png](RL%20Offline%20RL%203fb412ab82474b19b1506a659d543f0e/Never-Ending-Storage.png)

    - use human-in-the-loop for *reward sketching*
- important aspects for Batch RL
    - whether the data is generated from a non-stationary or a stationary policy. Furthermore, how diverse is the dataset
    - essential to figure out ways that an imitator can outperform the expert

---

---

## Papers

### Offline Reinforcement Learning: Tutorial, Review,
and Perspectives on Open Problems

link: [https://arxiv.org/pdf/2005.01643.pdf](https://arxiv.org/pdf/2005.01643.pdf) 

---

### Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems

link: [http://arxiv.org/abs/2005.01643](http://arxiv.org/abs/2005.01643) 

Summary

---

### Off-Policy Deep Reinforcement Learning without Exploration

link: [http://arxiv.org/abs/1812.02900](http://arxiv.org/abs/1812.02900) 

code: [https://github.com/sfujim/BCQ](https://github.com/sfujim/BCQ)

Summary: 

- 

---

### Benchmarking Batch Deep Reinforcement Learning Algorithms

link: [http://arxiv.org/abs/1910.01708](http://arxiv.org/abs/1910.01708)

---

### Quantile Regression DQN (QR-DQN)

---

### Random Ensemble Mixture (REM)

---

### Bootstrapping Error Accumulation Reduction Q-Learning (BEAR-QL)

---

### KL-Control

---

### Safe Policy Improvement with Baseline Bootstrapping DQN (SPIBB-DQN)

---

### Stabilizing Off-Policy Q-Learning via Bootstrapping
Error Reduction

link: [https://arxiv.org/pdf/1906.00949.pdf](https://arxiv.org/pdf/1906.00949.pdf)

---

### An Optimistic Perspective on Offline Reinforcement Learning

link: [https://arxiv.org/pdf/1907.04543.pdf](https://arxiv.org/pdf/1907.04543.pdf)

---

### IRIS: Implicit Reinforcement without Interaction at Scale for Learning Control from Offline Robot Manipulation Data

link: [https://arxiv.org/pdf/1911.05321.pdf](https://arxiv.org/pdf/1911.05321.pdf)

---

### Scaling data-driven robotics with reward sketching and batch reinforcement learning

link: [https://arxiv.org/pdf/1909.12200.pdf](https://arxiv.org/pdf/1909.12200.pdf)

---

---

## References

- Minimax-Optimal Off-Policy Evaluation via Linear Regression

[rl-202008-OPE.pdf](https://drive.google.com/file/d/1FpOy2C5mm6pWaaVehvmwiHdQx_L-FgHn/view)