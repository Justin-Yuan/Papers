# RL: Safe RL

---

## Safe RL

link: [https://web.stanford.edu/class/cs234/slides/2017/cs234_guest_lecture_safe_rl.pdf](https://web.stanford.edu/class/cs234/slides/2017/cs234_guest_lecture_safe_rl.pdf) 

![RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Screen_Shot_2020-08-22_at_4.43.54_PM.png](RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Screen_Shot_2020-08-22_at_4.43.54_PM.png)

![RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Screen_Shot_2020-08-22_at_4.45.43_PM.png](RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Screen_Shot_2020-08-22_at_4.45.43_PM.png)

![RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Screen_Shot_2020-08-22_at_5.02.36_PM.png](RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Screen_Shot_2020-08-22_at_5.02.36_PM.png)

![RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Screen_Shot_2020-08-22_at_5.06.48_PM.png](RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Screen_Shot_2020-08-22_at_5.06.48_PM.png)

![RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Screen_Shot_2020-08-22_at_5.08.07_PM.png](RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Screen_Shot_2020-08-22_at_5.08.07_PM.png)

![RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Screen_Shot_2020-08-22_at_5.09.56_PM.png](RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Screen_Shot_2020-08-22_at_5.09.56_PM.png)

---

## Safety in Sequential Decision Making

link: [https://cnls.lanl.gov/external/gsslides2019/Chow.pdf](https://cnls.lanl.gov/external/gsslides2019/Chow.pdf) 

![RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Screen_Shot_2020-08-22_at_5.39.59_PM.png](RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Screen_Shot_2020-08-22_at_5.39.59_PM.png)

![RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Screen_Shot_2020-08-22_at_5.45.12_PM.png](RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Screen_Shot_2020-08-22_at_5.45.12_PM.png)

![RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Screen_Shot_2020-08-22_at_6.02.32_PM.png](RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Screen_Shot_2020-08-22_at_6.02.32_PM.png)

![RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Screen_Shot_2020-08-22_at_6.05.28_PM.png](RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Screen_Shot_2020-08-22_at_6.05.28_PM.png)

![RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Screen_Shot_2020-08-22_at_6.06.47_PM.png](RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Screen_Shot_2020-08-22_at_6.06.47_PM.png)

![RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Screen_Shot_2020-08-22_at_6.12.27_PM.png](RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Screen_Shot_2020-08-22_at_6.12.27_PM.png)

![RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Screen_Shot_2020-08-22_at_6.12.37_PM.png](RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Screen_Shot_2020-08-22_at_6.12.37_PM.png)

![RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Screen_Shot_2020-08-22_at_6.12.57_PM.png](RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Screen_Shot_2020-08-22_at_6.12.57_PM.png)

![RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Screen_Shot_2020-08-22_at_6.13.18_PM.png](RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Screen_Shot_2020-08-22_at_6.13.18_PM.png)

---

## Trial without Error: Towards Safe RL with Human Intervention

link: [https://owainevans.github.io/blog/hirl_blog.html](https://owainevans.github.io/blog/hirl_blog.html)

![RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Screen_Shot_2020-08-22_at_6.31.27_PM.png](RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Screen_Shot_2020-08-22_at_6.31.27_PM.png)

![RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Screen_Shot_2020-08-22_at_6.31.43_PM.png](RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Screen_Shot_2020-08-22_at_6.31.43_PM.png)

![RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Screen_Shot_2020-08-22_at_6.31.57_PM.png](RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Screen_Shot_2020-08-22_at_6.31.57_PM.png)

---

## Towards Safe Reinforcement Learning

link: [https://medium.com/@harshitsikchi/towards-safe-reinforcement-learning-88b7caa5702e](https://medium.com/@harshitsikchi/towards-safe-reinforcement-learning-88b7caa5702e)

Generally, 2 major ways 

- Change the optimization criteria

    ![RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Screen_Shot_2020-08-22_at_6.42.25_PM.png](RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Screen_Shot_2020-08-22_at_6.42.25_PM.png)

    ![RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Screen_Shot_2020-08-22_at_6.42.39_PM.png](RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Screen_Shot_2020-08-22_at_6.42.39_PM.png)

    ![RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Screen_Shot_2020-08-22_at_6.43.24_PM.png](RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Screen_Shot_2020-08-22_at_6.43.24_PM.png)

- Change the exploration process

    ![RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Screen_Shot_2020-08-22_at_6.46.49_PM.png](RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Screen_Shot_2020-08-22_at_6.46.49_PM.png)

    ![RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Screen_Shot_2020-08-22_at_6.47.00_PM.png](RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Screen_Shot_2020-08-22_at_6.47.00_PM.png)

incorporating constraints in RL 

- soft constraints
    - [Constrained Policy Optimization](https://arxiv.org/pdf/1705.10528.pdf) (CPO)
        - algorithm’s update rule projected the policy to a safe feasibility set in each iteration
        - shown to keep the policy within constraints **in expectation**. It is unsuitable for use-cases, where safety must be ensured for all visited states and during training
    - [Safe, Multi-Agent, Reinforcement Learning for Autonomous Driving](https://arxiv.org/pdf/1610.03295.pdf)

- hard constraints
    - [Trial without Error: Towards Safe Reinforcement Learning via Human Intervention](https://arxiv.org/pdf/1707.05173.pdf)
    - [Safe Exploration in Continuous Action Spaces](https://arxiv.org/pdf/1801.08757.pdf)
        - directly add to the policy a safety layer that analytically solves an action correction formulation per each state

---

## Safe model-based learning for robot control

link: [https://kgatsis.github.io/learning_for_control_workshop_CDC2018/assets/slides/Berkenkamp_CDC18.pdf](https://kgatsis.github.io/learning_for_control_workshop_CDC2018/assets/slides/Berkenkamp_CDC18.pdf) 

![RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Screen_Shot_2020-08-22_at_8.14.10_PM.png](RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Screen_Shot_2020-08-22_at_8.14.10_PM.png)

![RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Screen_Shot_2020-08-22_at_8.14.22_PM.png](RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Screen_Shot_2020-08-22_at_8.14.22_PM.png)

![RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Screen_Shot_2020-08-22_at_8.16.43_PM.png](RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Screen_Shot_2020-08-22_at_8.16.43_PM.png)

![RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Screen_Shot_2020-08-22_at_8.21.50_PM.png](RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Screen_Shot_2020-08-22_at_8.21.50_PM.png)

---

## Tutorial on Safe Reinforcement Learning

link: [https://las.inf.ethz.ch/files/ewrl18_SafeRL_tutorial.pdf](https://las.inf.ethz.ch/files/ewrl18_SafeRL_tutorial.pdf) 

![RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Untitled.png](RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Untitled.png)

### specifying safe behavior

![RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Untitled%201.png](RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Untitled%201.png)

- expected safety

![RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Untitled%202.png](RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Untitled%202.png)

- safety variance

![RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Untitled%203.png](RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Untitled%203.png)

- Value at Risk

![RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Untitled%204.png](RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Untitled%204.png)

- Conditional Value at Risk

![RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Untitled%205.png](RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Untitled%205.png)

- Worst-case

![RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Untitled%206.png](RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Untitled%206.png)

![RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Untitled%207.png](RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Untitled%207.png)

### imitation learning

- data aggregation

![RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Untitled%208.png](RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Untitled%208.png)

- policy aggregation

![RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Untitled%209.png](RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Untitled%209.png)

- safe imitation learning

![RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Untitled%2010.png](RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Untitled%2010.png)

### Prior knowledge as backup for learning

![RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Untitled%2011.png](RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Untitled%2011.png)

### Safety as improvement in performance (Expected safety)

- off-policy policy evaluation

![RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Untitled%2012.png](RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Untitled%2012.png)

### Safe reinforcement learning

![RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Untitled%2013.png](RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Untitled%2013.png)

### SafeOPT

![RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Untitled%2014.png](RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Untitled%2014.png)

### safe exploration

![RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Untitled%2015.png](RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Untitled%2015.png)

### safe model-based RL

![RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Untitled%2016.png](RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Untitled%2016.png)

![RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Untitled%2017.png](RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Untitled%2017.png)

### future work

![RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Untitled%2018.png](RL%20Safe%20RL%205251a2a82b9542848806c00013f413f8/Untitled%2018.png)

---

## 

---

## Papers

### A Comprehensive Survey on Safe Reinforcement Learning

link: [https://www.jmlr.org/papers/volume16/garcia15a/garcia15a.pdf](https://www.jmlr.org/papers/volume16/garcia15a/garcia15a.pdf)  

---

### Safe and efficient off-policy reinforcement learning

link: [https://arxiv.org/pdf/1606.02647.pdf](https://arxiv.org/pdf/1606.02647.pdf) 

---

### Reachability-Based Safe Learning with Gaussian Processes

link: [http://www.ece.ubc.ca/~kaynama/papers/CDC2014_safelearning.pdf](http://www.ece.ubc.ca/~kaynama/papers/CDC2014_safelearning.pdf) 

---

### Benchmarking Safe Exploration in Deep Reinforcement Learning

link: [https://cdn.openai.com/safexp-short.pdf](https://cdn.openai.com/safexp-short.pdf) 

---

### Constrained Policy Optimization

link: [http://arxiv.org/abs/1705.10528](http://arxiv.org/abs/1705.10528)

code: [https://github.com/jachiam/cpo](https://github.com/jachiam/cpo)

---

## Tools

- safety gym ([https://github.com/openai/safety-gym](https://github.com/openai/safety-gym))
- safety gym agents ([https://github.com/openai/safety-starter-agents](https://github.com/openai/safety-starter-agents))
- safeRL repo ([https://github.com/hari-sikchi/safeRL](https://github.com/hari-sikchi/safeRL))
-