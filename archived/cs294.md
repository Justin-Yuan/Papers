### Berkeley CS294 Deep RL [link](http://rail.eecs.berkeley.edu/deeprlcourse/)

#### Lec 2 
Imitation learning --> supervised learning 

DAgger: Dataset Aggregation
- Add more on-policy data
- addresses the problem of distributional “drift”

fail to fit the expert 
- Non-Markovian behavior
    - use RNN to leverage full history 
- Multimodal behavior 
    - Output mixture of Gaussians
    - Latent variable models
        - Conditional variational autoencoder
        - Normalizing flow/realNVP
        - Stein variational gradient descent
    - Autoregressive discretization


#### Lec 4 Intro to RL 
Markov Decision Process 
- S, A discrete or continuous 
- T transition operator, a tensor 
- reward function r(s_t, a_t)
- M = {S, A, T, r}

Partially Observed MDP 
- O discrete or continuous 
- E emissoin probability p(o_t|s_t)
- M = {S, A, T, O, E, r}

In RL, we almost always care about expectations --> reward smooth in parameters 

Stochastic systems 

Q-function % Value function 

Types of RL algorithms 
- Policy gradients: directly differentiate the above objective
- Value-based: estimate value function or Q-function of the optimal policy (no explicit policy)
- Actor-critic: estimate value function or Q-function of the current policy, use it to improve policy
- Model-based RL: estimate the transition model, and then…
    - Use it for planning (no explicit policy)
    - Use it to improve a policy
    - Something else

Sample efficiency = how many samples do we need to get a good policy?
    - Most important question: is the algorithm off policy?
        - Off policy: able to improve the policy without generating new samples from that policy
        - On policy: each time the policy is changed, even a little bit, we need to generate new samples

Examples algorithms 
- Value function fitting methods
    - Q-learning, DQN
    - Temporal difference learning
    - Fitted value iteration
- Policy gradient methods
    - REINFORCE
    - Natural policy gradient
    - Trust region policy optimization
- Actor-critic algorithms
    - Asynchronous advantage actor-critic (A3C)
    - Soft actor-critic (SAC)
- Model-based RL algorithms
    - Dyna
    - Guided policy search


#### Lec 5 Policy Gradients 

- reduce variance --> causality 
- baselines

PG is on-policy --> off-policy learning & importance sampling 

Covariant/natural policy gradient
    - parameterizatoin independent divergence measure, usually KL-divergence 
    - Fisher information matrix --> 2nd order approximate to KL divergence 
    - conjugage gradient 


#### Lec 6 Actor-Critic

advantage A(a_t, s_t) = Q(a_t, s_t) - V(s_t)
advantage as value function fitting A(a_t, s_t) = r(a_t, s_t) + V(s_t+1) - V(s_t)

policy evaluation --> Monte Carlo, Fitting value function to policy

train V(s) by supervised regressoin on Bellman error or bootstrapping using previous fitted value function 

discount factors (0.99 works well) --> variance reduction 

batch AC & online AC 

policy and value function networks 
- two network design 
- shared network design with two heads 

Critics as state-dependent baselines
Control variates: action-dependent baselines

Eligibility traces & n-step returns (to control bias/variance tradeoff)
Generalized advantage estimation --> Weighted combination of n-step returns


#### Lec 7 Value function methods 

policy iteration 
- evaluate A(s, a)
- set pi <- pi' 

pi' = 1 if a_t = argmax_(a_t) A(s_t, a_t) otherwise 0 
A(s, a) = r(s, a) + gamma * E[V(s')] - V(s)

Dynamic programming 
bootstrapped update
    - deterministic policy a = pi(s)
    - policy evaluation V(s) <- r(s, pi(s)) + gamma * E_s'~p(s'|s, pi(s))[V(s')]

Value iteration 
- set Q(s, a) <- r(s, a) + gamma * E_s'~p(s'|s, a)[V(s')]
- set V(s) <- max_a Q(s, a)

replace A(s, a) with Q(s, a) = r(s, a) + gamma * E_s'~p(s'|s, a)[V(s')]
skip policy and compute value directly 

fitted Value iteration (V(s) as neural network)
- set y_i <- max_(a_i)(r(s_i, a_i) + gamma * E[V_phi(s')])
- set phi <- argmin_phi 1/2 Sum_i ||V_phi(s_i) - y_i||^2

if dont know transition dynamics 
- policy iteration 
    - evaluate Q(s, a)
    - set pi <- pi' 
    - policy improvement
        - pi'(a_t|s_t) = 1 if a_t = argmax_(a_t) Q(s_t, a_t) else 0 
    - policy evaluation
        - Q(s, a) <- r(s, a) + gamma * E_s'~p(s'|s,a)[Q(s', pi(s'))]  fit this using samples 
- fitted Q iteration 
    - approximate E[V(s')] ~= max_a' Q_phi(s'_i, a'_i) 
    
Exploration with Q-learning
- epsilon-greedy
- Boltzmann exploration  pi(a_t|s_t) ~ exp(Q_phi(s_t, a_t)) 

Value function Learning theory 
- an operator B: BV = max_a r_a + gamma * T_a V  (backup, Bellman operator, contraction)
- V* is a fixed point of B, always exists, unique and corresponds to the optimal policy 

Non-tabluar value function learning 
- Omega: all value functions represented by neural nets 
- new operator Pi: PiV = argmin_V' in Omega 1/2 Sum||V'(s) - V(s)||^2 
- Pi is a projection onto Omega, contraction 
- but PiB is not a contraction -> value iteration converges but fitted value iteration does not 


#### Lec 8 Advanced value functions 

online Q-iteration/Q-learning 
- sequential states are strongly correlated 
- target value is always changing 

replay buffers 
target networks 

DQN = Q-learning with replay buffer and target network

alternative target network -> use Polyak averaging phi' <- tao * phi' + (1-tao) * phi,  tao = 0.999 works well 

Overestimation in Q-learning 
- for 2 random variables X1, X2 -> E[max(X1, X2)] >= max(E[X1], E[X2])
- dont use the same network to choose the action and evaluate value 
- double Q-learning 

Q-learning with N-step returns
- dynamically choose N 
- importance sampling 

Q-learning with continuous actions
- problematic "max" operation 
- use optimization
- use function class that is easy to optimize
- learn an approximate maximizer
    - train anothe network s.t. mu_theta(s) ~= argmax_a Q_phi(s, a)
    - by solving theta <- argmax_theta Q_phi(s, mu_theta(s))

Advanced tips 
- Bellman error gradients can be big; clip gradients or user Huber loss
- Double Q-learning helps a lot in practice, simple and no downsides
- N-step returns also help a lot, but have some downsides
- Schedule exploration (high to low) and learning rates (high to low), Adam optimizer can help too
- Run multiple random seeds, it’s very inconsistent between runs


#### Advanced Policy Gradients 

Policy gradient as policy iteration
- Correct thing to do is optimize expected advantage under new
policy state distribution
- Doing this under old policy state distribution optimizes a bound, if the policies are close enough
- Results in constrained optimization problem

Review 
- First order approximation to objective = gradient ascent
- Regular gradient ascent has the wrong constraint
- Taylor expansion of KL-divergence = natural gradient
- Practical algorithms
    - Natural policy gradient
    - Trust region policy optimization


#### Lec 10 Optimal Control and Planning


#### Lec 11


#### Lec 12


#### Lec 13


#### Lec 14 Variational Inference and Generative Models

