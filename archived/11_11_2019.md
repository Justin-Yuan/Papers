# Topics for the day 

<!-- ***************************************************** -->

# List of works
- [Multiple Futures Prediction](#1)
- [Dynamics-Aware Unsupervised Discovery of Skills](#2)
- [Experience Sharing Between Cooperative Reinforcement Learning Agents](#3)
- [Graph Neural Tangent Kernel: Fusing Graph Neural Networks with Graph Kernels](#4)
- [Neural Tangent Kernel: Convergence and Generalization in Neural Networks](#5)
- [Differentiable Convex Optimization Layers](#6)
- [SMOOTH MARKETS: A BASIC MECHANISM FOR ORGANIZING GRADIENT-BASED LEARNERS](#7)
- [A THEORY OF USABLE INFORMATION UNDER COMPUTATIONAL CONSTRAINTS](#8)
- [BACKPACK: PACKING MORE INTO BACKPROP](#9)

<!-- ***************************************************** -->

# Work summaries

<a name="1"></a> 
Multiple Futures Prediction
<https://arxiv.org/pdf/1911.00997.pdf>

- a probabilistic framework that efficiently learns latent variables to jointly model the multi-step future motions of agents in a scene
- dynamic attention-based state encoder
- use for planning via computing a conditional probability density over the trajectories of other agents given a hypothetical rollout of the ‘self’ agent
    

--- 
<a name="2"></a> 
Dynamics-Aware Unsupervised Discovery of Skills
<https://arxiv.org/pdf/1907.01657.pdf>

- combine model-based learning with model-free learning of primitives that make model-based planning easy
- simultaneously discovers predictable behaviors and learns their dynamics
- can leverage continuous skill spaces
- demonstrate zero-shot planning in the learned latent space, outperforms standard MBRL and model-free goal-conditioned RL


---
<a name="3"></a> 
Experience Sharing Between Cooperative Reinforcement Learning Agents
<https://arxiv.org/pdf/1911.02191.pdf>

- experience sharing (ES) between autonomous and independent agents could accelerate leraning in cooperative multiagent settings 
- propose 3 methods for selecting experiences 
- Focused ES, prioritizes unexplored regions in state space; Prioritized ES, temporal difference error used as a measure of priority; Focused Prioritized ES, combines both
- randomly selected experiences between 2 Deep Q-Network agents doesn't improve over single agent baseline 


--- 
<a name="4"></a> 
Graph Neural Tangent Kernel: Fusing Graph Neural Networks with Graph Kernels
<https://arxiv.org/pdf/1905.13192.pdf>
code: <https://github.com/KangchengHou/gntk>

- graph kernels (GK) has limited expressiveness compared to graph neural networks (GNN), but GNN trianing is non-convex and harder to train
- Graph Neural Tangent Kernel (GNTK), correspond to infinitely wide multi-layer GNN, provably learn a class of smooth functions on graphs


---
<a name="5"></a> 
Neural Tangent Kernel: Convergence and Generalization in Neural Networks
<https://arxiv.org/pdf/1806.07572.pdf>
code: <https://github.com/google/neural-tangents>

- at initialization, neural networks are equivalent to Gaussian processes in the infinite-width limit --> connection to kernel methods 
- during training, network function follows the kernel gradient of the functional cost w.r.t a new kernel: Neural Tangent Kernel (NTK)
- study training of network in function space instead of parameter space 
- convergence of training can be related to positive-definiteness of the limiting NTK 


--- 
<a name="6"></a> 
Differentiable Convex Optimization Layers
<http://web.stanford.edu/~boyd/papers/pdf/diff_cvxpy.pdf>
code: <https://github.com/cvxgrp/cvxpylayers>

- embed differentiable optimization problems (problems whose solutions can be backpropagated through) as layers within deep leraning architectures 
- disciplined parametrized programming, show that it can be represented as the composition of an affine map from parameters to problem data, a solver, and an affine map from the solver’s solution to a solution of the original problem (affine-solver-affine)


---
<a name="7"></a> 
SMOOTH MARKETS: A BASIC MECHANISM FOR ORGANIZING GRADIENT-BASED LEARNERS
<https://openreview.net/pdf?id=B1xMEerYvB>

- important to understand and control how learning algorithms interact (as in general n-player games)
- introduce smooth markets (SM-games), n-player games with pairwise zero sum interactions


---
<a name="8"></a> 
A THEORY OF USABLE INFORMATION UNDER COMPUTATIONAL CONSTRAINTS
<https://openreview.net/pdf?id=r1eBeyHFDH>

- framework for reasoning about information in complex systems, based on variational extension of Shannon's information theory
- predictive F-information, encompasses mutual information; with computational constraints, it can be reliably estimated from data wtih PAC-style guarantees
- prove empirically predictivec F-informatin is more effective than mutual information for structure learning and fair representation learning 


--- 
<a name="9"></a> 
BACKPACK: PACKING MORE INTO BACKPROP
<https://openreview.net/pdf?id=BJlrF24twB>
code: <https://toiaydcdyywlhzvlob.github.io/backpack/>

- framework on PyTorch to extend backpropagation to extract additional information from first- and second-order derivatives (e.g. variance, hessian)