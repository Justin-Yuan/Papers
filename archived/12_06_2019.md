# Topics for the day 

compressed sensing, differential games, information bottleneck, mutual information estimation 

<!-- ***************************************************** -->

# List of works
- [LOGAN: LATENT OPTIMISATION FOR GENERATIVE ADVERSARIAL NETWORKS](#1)
- [Deep Compressed Sensing](#2)
- [Generalization in Reinforcement Learning with Selective Noise Injection and Information Bottleneck](#3)
- [Mutual Information Neural Estimation](#4)
- [LEARNING DEEP REPRESENTATIONS BY MUTUAL INFORMATION ESTIMATION AND MAXIMIZATION](#5)

<!-- ***************************************************** -->

# Work summaries

<a name="1"></a> 
LOGAN: LATENT OPTIMISATION FOR GENERATIVE ADVERSARIAL NETWORKS
<https://arxiv.org/abs/1912.02390>

- new form of latent optimization to improve adversarial dynamics by enhancing interactions between discriminator and generator
- imporve latent optimization with efficient second-order updates

### Methodology 
- GAN formulation 
$$
min_{\theta_D}max_{\theta_G} E_{x \sim p(x)}[h_D(D(x;\theta_D))] + E_{z \sim p(z)}[ h_G(D( G(z;\theta_G); \theta_D)) ] 
$$
- compare with BigGAN  (with residual block, regularization and self-attention)
- apply **Symplectic Gradient Adjustment** (SGA) to GAN
- latent optimization efficiently couples the gradients of the discriminator and generator, but using the much lower-dimensional latent source z which makes the adjustment scalable 
- use natural gradient descent for latent optimization 


--- 
<a name="2"></a> 
Deep Compressed Sensing
<https://arxiv.org/pdf/1905.06723.pdf>

- recover sparse signals from compressed measurements 
- restriction on assumption of sparsity and costly reconstruction process 
- jointly train a generator and the optimization process for reconstruction via meta-learning 
- GAN as a special case in family of CS models, improve GANs using gradient information from discriminator 


--- 
<a name="3"></a> 
Generalization in Reinforcement Learning with Selective Noise Injection and Information Bottleneck
<https://arxiv.org/pdf/1910.12911.pdf>

- 


--- 
<a name="4"></a> 
Mutual Information Neural Estimation
<https://arxiv.org/pdf/1801.04062.pdf>

- in contrast to correlation, mutual information captures non-linear statistical dependencies between variables, act as measure of true dependence 
- mutual information as the Kullback-Leibler divergence between the joint distribution and the product of the marginals $I(X;Z) = D_{KL}(P_{XZ} || P_{X} \bigotimes P_{Z})$
- GAN cast esetimation of f-divergences as part of an adversarial game between competing deep neural networks 
- **Donsker-Varadhan representation**, **f-divergence representation**, **Fenchel duality** 
- neural information measure $I_{\Theta}(X,Z) = sup_{\theta \in \Theta}E_{P_{XZ}}[T_{\theta}] - log(E_{P_{X} \bigotimes P_{Z}}[e^{T_{\theta}}])$
- MINE is strongly consistent 
- alleviate GAN mode collapse by maximiizing MI between generated samples and the code (conditioned on)
- MI help to impplement information bottleneck, yield representation Z from X that contributes to prediction of Y, (X --> Z --> Y) 
$L[q(Z|X)] = H(Y|Z) + \beta I(X, Z)$


--- 
<a name="5"></a> 
LEARNING DEEP REPRESENTATIONS BY MUTUAL INFORMATION ESTIMATION AND MAXIMIZATION
<https://arxiv.org/pdf/1808.06670.pdf>

- unsupervised learning of representations by maximizing mutual informatoin between an input and the output of a deep neural network encoder 
- control characteristics of the representation by mathcing to a prior distribution adversarially 
- structure matters: maximizing the average MI between the representation and local regions of the input can improve representation quality 
- new measures of representation quality, one based on MINE and a neural dependency measure (NDM)