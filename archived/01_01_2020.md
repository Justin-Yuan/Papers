 # Topics for the day 

 graph network, graph generation, model-based RL, soft RL

<!-- ***************************************************** -->

# List of works
- [Graph Networks as Learnable Physics Engines for Inference and Control](#1)
- [GraphRNN: Generating Realistic Graphs with Deep Auto-regressive Models](#2)
- [Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models](#3)
- [Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor](#4)
- [Efficient Graph Generation with Graph Recurrent Attention Networks](#5)

<!-- ***************************************************** -->

# Work summaries

<a name="1"></a> 
Graph Networks as Learnable Physics Engines for Inference and Control
<https://arxiv.org/pdf/1806.01242v1.pdf>

- probabilistic NN as a network whose output neurons simply parameterize a probability distribution function, capturing aleatoric uncertainty
- inference model can perform system identification
- Some key future directions include using our approach for control in real-world settings, supporting simulation-to-real transfer via pre-training models in simulation, extending our models to handle stochastic environments, and performing system identification over the structure of the system as well as the parameters. Our approach may also be useful within imagination-based planning frameworks (Hamrick et al., 2017; Pascanu et al., 2017), as well as integrated architectures with GN-like policies (Wang et al., 2018).


--- 
<a name="2"></a> 
GraphRNN: Generating Realistic Graphs with Deep Auto-regressive Models
<https://arxiv.org/pdf/1802.08773.pdf>

- modeling and sampling from distributions over graphs is challenging due to the non-unique, high-dimensional nature of graphs and non-local dependencies between edges in a given graph
- decompose grapn generation into a sequence of node and edge formations, conditioned on the graph structure generated so far 
- introduce benchmark suite of datasets, baselines and evaluation metrics based on MMD between sets of graphs 
- GraphRNN reduces MMD over baselines, achieve high log-likelihood scores on held-out data 


--- 
<a name="3"></a> 
Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models
<https://arxiv.org/pdf/1805.12114.pdf>

- employ uncertainty-aware dynamics models
- probabilistic ensembles with trajectory sampling (PETS that combines uncertainty-aware deep network dynamics models with sampling-based uncertainty propagation
-  matches the asymptotic performance of model-free algorithms on several challenging benchmark tasks
- dynamics model is reward-independent and therefore can generalize to new tasks in the same environment, and it can easily benefit from all of the advances in deep supervised learning to utilize high-capacity models
- although MBRL learns more quickly, it tends to converge to less optimal solutions 
- model capacity is a critical ingredient in the success of MBRL methods: while efficient models such as Gaussian processes can learn extremely quickly, they struggle to represent very complex and discontinuous dynamical systems
-  two distinct classes of uncertainty: aleatoric (inherent system stochasticity) and epistemic (subjective uncertainty, due to limited data)
- probabilistic NN as a network whose output neurons simply parameterize a probability distribution function, capturing aleatoric uncertainty
- probabilistic NN as a network whose output neurons simply parameterize a probability distribution function, capturing aleatoric uncertainty


--- 
<a name="4"></a> 
Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor
<https://arxiv.org/pdf/1801.01290.pdf>



--- 
<a name="5"></a> 
Efficient Graph Generation with Graph Recurrent Attention Networks
<https://arxiv.org/pdf/1910.00760.pdf>

- generates graphs one block of nodes and associated edges at a time
- reduces the dependency on node ordering but also bypasses the long-term bottleneck caused by the sequential nature of RNNs
- parameterize the output distribution per block using a mixture of Bernoulli,
- handle node orderings in generation by marginalizing over a family of canonical orderings
- up to 5K nodes with good quality.
