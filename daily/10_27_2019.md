# Topics for the day 

mutual information, model-based RL

<!-- ***************************************************** -->

# List of works
- [ROBUST DOMAIN RANDOMIZATION FOR REINFORCEMENT LEARNING](#1)
- [MIM: Mutual Information Machine](#2)
- [Model Imitation for Model-Based Reinforcement Learning](#3)
- [Accelerating Natural Gradient with Higher-Order Invariance](#4)
- [DEEP GRAPH INFOMAX](#5)
- [LEARNING DEEP REPRESENTATIONS BY MUTUAL INFORMATION ESTIMATION AND MAXIMIZATION](#6)
- [DEEP VARIATIONAL INFORMATION BOTTLENECK](#7)


<!-- ***************************************************** -->

# Work summaries

<a name="1"></a> 
ROBUST DOMAIN RANDOMIZATION FOR REINFORCEMENT LEARNING
<https://arxiv.org/pdf/1910.10537.pdf>

- domain randomization on RL is inefficient, high variance across domains
- formalize domain randomization problem, Lipschitz constant of agent policy over randomization parameters provide upper bound on agent robustness to variation in environment
- minimize policy's Lipschitz constant lead to low variance and better generalization 


--- 
<a name="2"></a> 
MIM: Mutual Information Machine
<https://arxiv.org/pdf/1910.03175.pdf>

- autoencoder model for learning joint distributions over observations and latent states
- learn useful representations for downstream tasks
- provide data log likelihood 


---
<a name="3"></a> 
Model Imitation for Model-Based Reinforcement Learning
<https://arxiv.org/pdf/1909.11821.pdf>

- model-based RL learnt model fails due to estimation error, affect sample complexity 
- propose to learn synthesized model by matching distributions of multi-step rollouts sampled from synthesized model and real ones via WGAN 
- in summary, use GAN to learn dynamics model 


--
<a name="4"></a>
Accelerating Natural Gradient with Higher-Order Invariance
<https://arxiv.org/pdf/1803.01273.pdf>

- natural gradient invariant to arbitrary differentiable reparameterizations of model 
- use higher-order integrators and geodesic corrections
to obtain more invariant optimization trajectories


--- 
<a name="5"></a>
DEEP GRAPH INFOMAX
<https://arxiv.org/pdf/1809.10341.pdf>

- unsupervised learning for node representation within graph-structured data
- maxmimize mutual information between patch representations and high-level summaries of graphs 
- do not rely on random walk objectives

---
<a name="6"></a>
LEARNING DEEP REPRESENTATIONS BY MUTUAL INFORMATION ESTIMATION AND MAXIMIZATION
<https://openreview.net/pdf?id=Bklr3j0cKX>

- Deep InfoMax, maximize mutual information between network input and output
- match a prior distribution adversarily 

---
<a name="7"></a>
DEEP VARIATIONAL INFORMATION BOTTLENECK
<https://arxiv.org/pdf/1612.00410.pdf>

- variational approximation to information bottleneck (IB), use neural network to parameterize IB, use reparameterization trich 
- better generalization and robustness to adversarial attack 

