# Topics for the day 

RL exploration via curiosity 

<!-- ***************************************************** -->

# List of works
- [Self-Supervised Exploration via Disagreement](#1)
- [EPISODIC CURIOSITY THROUGH REACHABILITY](#2)
- [IMPLICIT GENERATIVE MODELING FOR EFFICIENT EXPLORATION](#3)

<!-- ***************************************************** -->

# Work summaries

<a name="1"></a> 
Self-Supervised Exploration via Disagreement
<https://pathak22.github.io/exploration-by-disagreement/resources/icml19.pdf>
code: <https://pathak22.github.io/exploration-by-disagreement/>

- exploration gets stuch in environments with stochastic dynamics or inefficient for scaling up to real robotics 
- inspired by active learning, train ensemble of dynamics models and incentivize agent to explore such that disagreement of ensembles is maximized (e.g. variance of L1 between predicted next states and actual next state)
- learn skills in self-supervised manner **without any external reward**
- use disagreement objective to optimize agent policy in a differentiable manner, without RL so sample-efficient 
- test on stochastic-Atari, Mujoco and Unity 


--- 
<a name="2"></a> 
EPISODIC CURIOSITY THROUGH REACHABILITY
<https://arxiv.org/pdf/1810.02274.pdf>
code: <https://github.com/google-research/episodic-curiosity>

- derive novelty bonus, use episodic memory, the current observation is compared with observations in memory
- comparison is done baesd on how many environment steps it takes to reach the current observation from those in memory
- overcome "couch-potato" issues, test on VizDoom, DMLab and Mujoco 
- Siamese comparator network for reachability, trained on sequence of observations, within k steps are positive examples, otherwise negatives 
- compare with baseline **ICM**


--- 
<a name="3"></a> 
IMPLICIT GENERATIVE MODELING FOR EFFICIENT EXPLORATION
<https://arxiv.org/pdf/1911.08017.pdf>

- model uncertainty estimation as intrinsic reward for exploration, estimate a Bayesian uncertainty of the agent's belief of environment dynamics
- each random draw from the implicit generative model is a neural net that instantiates the dynamic function, hence multiple draws approximate the posterior
- variance in future prediction based on this posterior is used as intrinsic reward
- train generative model based on amortized **Stein Variational Gradient Descent**
- compare to baseline using ensemble of dynamic models 

