# Topics for the day 

<!-- ***************************************************** -->

# List of works
- [Hyperbolic Graph Neural Networks](#1)
- [Generalization of Reinforcement Learners with Working and Episodic Memory](#2)
- [Generalization in Reinforcement Learning with Selective Noise Injection and Information Bottleneck](#3)
- [Learning to Search with MCTSnets](#4)

<!-- ***************************************************** -->

# Work summaries

<a name="1"></a> 
Hyperbolic Graph Neural Networks
<https://arxiv.org/pdf/1910.12892.pdf>

- learn representations on Riemannian manifolds with differentiable exponential and logarithmic maps
- model structural properties of graphs, compare Euclidean and hyperbolic geometry 


--- 
<a name="2"></a> 
Generalization of Reinforcement Learners with Working and Episodic Memory
<https://arxiv.org/pdf/1910.13406.pdf>

- develop a comprehensive methodology to test different kinds of memory in an agent and how well they generalize at test time 
- construct a diverse set of memory tasks


---
<a name="3"></a> 
Generalization in Reinforcement Learning with Selective Noise Injection and Information Bottleneck
<https://arxiv.org/pdf/1910.12911.pdf>

- regularization techniques to RL, specifically on injection of noise, e.g. Dropout and BatchNorm 
- Selective Noise Injection, regularizes but mitigates the adverse effects it has on the gradient quality
- Information Bottleneck suited well to low-data regime at early RL training
- benchmark on *Coinrun*


--- 
<a name="4"></a> 
Learning to Search with MCTSnets
<https://arxiv.org/pdf/1802.04697.pdf>

- planning problems mostly solved by tree search algos, e.g. Monte-Carlo tree search (MCTS)
- MCTS use heuristic rules to control where the simulation traverses, what to evaluate in the states that are reached, and how to back-up those evaluations
- MCTSnet learns where, what and how to search, incorporates simulation-based search inside a neural net, by expanding, evaluating and backing-up a vector embedding
- learning is end-to-end